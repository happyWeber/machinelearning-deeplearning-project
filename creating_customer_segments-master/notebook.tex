
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{customer\_segments}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{机器学习纳米学位}\label{ux673aux5668ux5b66ux4e60ux7eb3ux7c73ux5b66ux4f4d}

\subsection{非监督学习}\label{ux975eux76d1ux7763ux5b66ux4e60}

\subsection{项目 3:
创建用户分类}\label{ux9879ux76ee-3-ux521bux5efaux7528ux6237ux5206ux7c7b}

    欢迎来到机器学习工程师纳米学位的第三个项目！在这个notebook文件中，有些模板代码已经提供给你，但你还需要实现更多的功能来完成这个项目。除非有明确要求，你无须修改任何已给出的代码。以\textbf{'练习'}开始的标题表示接下来的代码部分中有你必须要实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以\textbf{'TODO'}标出。请仔细阅读所有的提示！

除了实现代码外，你还\textbf{必须}回答一些与项目和你的实现有关的问题。每一个需要你回答的问题都会以\textbf{'问题
X'}为标题。请仔细阅读每个问题，并且在问题后的\textbf{'回答'}文字框中写出完整的答案。我们将根据你对问题的回答和撰写代码所实现的功能来对你提交的项目进行评分。

\begin{quote}
\textbf{提示：}Code 和 Markdown 区域可通过 \textbf{Shift + Enter}
快捷键运行。此外，Markdown可以通过双击进入编辑模式。
\end{quote}

    \subsection{开始}\label{ux5f00ux59cb}

在这个项目中，你将分析一个数据集的内在结构，这个数据集包含很多客户真对不同类型产品的年度采购额（用\textbf{金额}表示）。这个项目的任务之一是如何最好地描述一个批发商不同种类顾客之间的差异。这样做将能够使得批发商能够更好的组织他们的物流服务以满足每个客户的需求。

这个项目的数据集能够在\href{https://archive.ics.uci.edu/ml/datasets/Wholesale+customers}{UCI机器学习信息库}中找到.因为这个项目的目的，分析将不会包括'Channel'和'Region'这两个特征------重点集中在6个记录的客户购买的产品类别上。

运行下面的的代码单元以载入整个客户数据集和一些这个项目需要的Python库。如果你的数据集载入成功，你将看到后面输出数据集的大小。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} 检查你的Python版本}
        \PY{k+kn}{from} \PY{n+nn}{sys} \PY{k+kn}{import} \PY{n}{version\PYZus{}info}
        \PY{k}{if} \PY{n}{version\PYZus{}info}\PY{o}{.}\PY{n}{major} \PY{o}{!=} \PY{l+m+mi}{2} \PY{o+ow}{and} \PY{n}{version\PYZus{}info}\PY{o}{.}\PY{n}{minor} \PY{o}{!=} \PY{l+m+mi}{7}\PY{p}{:}
            \PY{k}{raise} \PY{n+ne}{Exception}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{请使用Python 2.7来完成此项目}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} 引入这个项目需要的库}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k+kn}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{visuals} \PY{k+kn}{as} \PY{n+nn}{vs}
        \PY{k+kn}{from} \PY{n+nn}{IPython.display} \PY{k+kn}{import} \PY{n}{display} \PY{c+c1}{\PYZsh{} 使得我们可以对DataFrame使用display()函数}
        
        \PY{c+c1}{\PYZsh{} 设置以内联的形式显示matplotlib绘制的图片（在notebook中显示更美观）}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        
        \PY{c+c1}{\PYZsh{} 载入整个客户数据集}
        \PY{k}{try}\PY{p}{:}
            \PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{customers.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{n}{data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Region}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Channel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{inplace} \PY{o}{=} \PY{n+nb+bp}{True}\PY{p}{)}
            \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Wholesale customers dataset has \PYZob{}\PYZcb{} samples with \PYZob{}\PYZcb{} features each.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{o}{*}\PY{n}{data}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{k}{except}\PY{p}{:}
            \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Dataset could not be loaded. Is the dataset missing?}\PY{l+s+s2}{\PYZdq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Wholesale customers dataset has 440 samples with 6 features each.

    \end{Verbatim}

    \subsection{分析数据}\label{ux5206ux6790ux6570ux636e}

在这部分，你将开始分析数据，通过可视化和代码来理解每一个特征和其他特征的联系。你会看到关于数据集的统计描述，考虑每一个属性的相关性，然后从数据集中选择若干个样本数据点，你将在整个项目中一直跟踪研究这几个数据点。

运行下面的代码单元给出数据集的一个统计描述。注意这个数据集包含了6个重要的产品类型：\textbf{'Fresh'},
\textbf{'Milk'}, \textbf{'Grocery'}, \textbf{'Frozen'},
\textbf{'Detergents\_Paper'}和
\textbf{'Delicatessen'}。想一下这里每一个类型代表你会购买什么样的产品。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} 显示数据集的一个描述}
        \PY{n}{display}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
               Fresh          Milk       Grocery        Frozen  \
count     440.000000    440.000000    440.000000    440.000000   
mean    12000.297727   5796.265909   7951.277273   3071.931818   
std     12647.328865   7380.377175   9503.162829   4854.673333   
min         3.000000     55.000000      3.000000     25.000000   
25%      3127.750000   1533.000000   2153.000000    742.250000   
50%      8504.000000   3627.000000   4755.500000   1526.000000   
75%     16933.750000   7190.250000  10655.750000   3554.250000   
max    112151.000000  73498.000000  92780.000000  60869.000000   

       Detergents_Paper  Delicatessen  
count        440.000000    440.000000  
mean        2881.493182   1524.870455  
std         4767.854448   2820.105937  
min            3.000000      3.000000  
25%          256.750000    408.250000  
50%          816.500000    965.500000  
75%         3922.000000   1820.250000  
max        40827.000000  47943.000000  
    \end{verbatim}

    
    \subsubsection{练习:
选择样本}\label{ux7ec3ux4e60-ux9009ux62e9ux6837ux672c}

为了对客户有一个更好的了解，并且了解代表他们的数据将会在这个分析过程中如何变换。最好是选择几个样本数据点，并且更为详细地分析它们。在下面的代码单元中，选择\textbf{三个}索引加入到索引列表\texttt{indices}中，这三个索引代表你要追踪的客户。我们建议你不断尝试，直到找到三个明显不同的客户。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} TODO：从数据集中选择三个你希望抽样的数据点的索引}
        \PY{n}{indices} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{200}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} 为选择的样本建立一个DataFrame}
        \PY{n}{samples} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{indices}\PY{p}{]}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{n}{drop} \PY{o}{=} \PY{n+nb+bp}{True}\PY{p}{)}
        \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Chosen samples of wholesale customers dataset:}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{display}\PY{p}{(}\PY{n}{samples}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Chosen samples of wholesale customers dataset:

    \end{Verbatim}

    
    \begin{verbatim}
   Fresh   Milk  Grocery  Frozen  Detergents_Paper  Delicatessen
0   7057   9810     9568    1762              3293          1776
1  11594   7779    12144    3252              8035          3029
2   3067  13240    23127    3941              9959           731
    \end{verbatim}

    
    \subsubsection{问题 1}\label{ux95eeux9898-1}

\emph{在你看来你选择的这三个样本点分别代表什么类型的企业（客户）？}对每一个你选择的样本客户，通过它在每一种产品类型上的花费与数据集的统计描述进行比较，给出你做上述判断的理由。

\textbf{提示：}
企业的类型包括超市、咖啡馆、零售商以及其他。注意不要使用具体企业的名字，比如说在描述一个餐饮业客户时，你不能使用麦当劳。

    \textbf{回答:}

0号样本代表零售商,Fresh靠近50\%,Milk在75\%，Grocery在75\%，Frozen在50\%，Detergents\_Paper在75\%，Delicatessen在75\%
都比较平均,量相对1号样本也不少一些。所以我推断是零售商的可能性比较大

1号样本代表超市，Fresh在75\%以上，Milk在75\%，Grocery在75\%，Frozen在75\%，Detergents\_Paper在75\%以上，Delicatessen在75\%以上，
大部分数据都超过平均值，没有哪样东西特别多，所以是一个大型超市的可能性比较大

2号样本代表咖啡馆,Milk、Grocery、Frozen和Detergents\_Paper的量超过各自的均值很多，Fresh和Delicatessen的量明显低于各自的均值，所以可以看出来该样本不属于什么都在经营的零售商和超市，更有可能是使用牛奶等物品较多的咖啡馆

    \subsubsection{练习:
特征相关性}\label{ux7ec3ux4e60-ux7279ux5f81ux76f8ux5173ux6027}

一个有趣的想法是，考虑这六个类别中的一个（或者多个）产品类别，是否对于理解客户的购买行为具有实际的相关性。也就是说，当用户购买了一定数量的某一类产品，我们是否能够确定他们必然会成比例地购买另一种类的产品。有一个简单的方法可以检测相关性：我们用移除了某一个特征之后的数据集来构建一个监督学习（回归）模型，然后用这个模型去预测那个被移除的特征，再对这个预测结果进行评分，看看预测结果如何。

在下面的代码单元中，你需要实现以下的功能： -
使用\texttt{DataFrame.drop}函数移除数据集中你选择的不需要的特征，并将移除后的结果赋值给\texttt{new\_data}。
-
使用\texttt{sklearn.model\_selection.train\_test\_split}将数据集分割成训练集和测试集。
-
使用移除的特征作为你的目标标签。设置\texttt{test\_size}为\texttt{0.25}并设置一个\texttt{random\_state}。
-
导入一个DecisionTreeRegressor（决策树回归器），设置一个\texttt{random\_state}，然后用训练集训练它。
- 使用回归器的\texttt{score}函数输出模型在测试集上的预测得分。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{} TODO：为DataFrame创建一个副本，用\PYZsq{}drop\PYZsq{}函数丢弃一个特征}
        \PY{k+kn}{from} \PY{n+nn}{sklearn.model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
        \PY{k+kn}{from} \PY{n+nn}{sklearn.tree} \PY{k+kn}{import} \PY{n}{DecisionTreeRegressor}
        \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{r2\PYZus{}score}
        
        \PY{n}{selectFeature} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Milk}\PY{l+s+s1}{\PYZsq{}}
        
        \PY{n}{new\PYZus{}data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{n}{selectFeature}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        
        \PY{n}{labels} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{selectFeature}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} TODO：使用给定的特征作为目标，将数据分割成训练集和测试集}
        \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{new\PYZus{}data}\PY{p}{,} \PY{n}{labels}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} TODO：创建一个DecisionTreeRegressor（决策树回归器）并在训练集上训练它}
        \PY{n}{regressor} \PY{o}{=} \PY{n}{DecisionTreeRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        
        \PY{n}{regressor}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
        
        \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{regressor}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} TODO：输出在测试集上的预测得分}
        \PY{n}{score} \PY{o}{=} \PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{p}{)}
        
        \PY{k}{print} \PY{n}{score}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
0.365725292736

    \end{Verbatim}

    \subsubsection{问题 2}\label{ux95eeux9898-2}

\emph{你尝试预测哪一个特征？预测的得分是多少？这个特征对于区分用户的消费习惯来说必要吗？}\\
\textbf{提示：} 决定系数（coefficient of determination）,
\texttt{R\^{}2},结果在0到1之间，1表示完美拟合，一个负的\texttt{R\^{}2}表示模型不能够拟合数据。

    \textbf{回答:} 我尝试的是Milk,预测的得分是0.365725292736,
得分较低,说明这个特征对于区分用户的消费习惯来说必要性较高，因为它可以通过其他数据拟合出来的可能性较低

    \subsubsection{可视化特征分布}\label{ux53efux89c6ux5316ux7279ux5f81ux5206ux5e03}

为了能够对这个数据集有一个更好的理解，我们可以对数据集中的每一个产品特征构建一个散布矩阵（scatter
matrix）。如果你发现你在上面尝试预测的特征对于区分一个特定的用户来说是必须的，那么这个特征和其它的特征可能不会在下面的散射矩阵中显示任何关系。相反的，如果你认为这个特征对于识别一个特定的客户是没有作用的，那么通过散布矩阵可以看出在这个数据特征和其它特征中有关联性。运行下面的代码以创建一个散布矩阵。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} 对于数据中的每一对特征构造一个散布矩阵}
        \PY{n}{pd}\PY{o}{.}\PY{n}{plotting}\PY{o}{.}\PY{n}{scatter\PYZus{}matrix}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{,} \PY{n}{diagonal} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kde}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_16_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{问题 3}\label{ux95eeux9898-3}

\emph{这里是否存在一些特征他们彼此之间存在一定程度相关性？如果有请列出。这个结果是验证了还是否认了你尝试预测的那个特征的相关性？这些特征的数据是怎么分布的？}

\textbf{提示：} 这些数据是正态分布(normally
distributed)的吗？大多数的数据点分布在哪？

    \textbf{回答:}

存在特征彼此之间有相关性 例如grocery和Milk

验证了我尝试预测的特征相关性

不符合正太分布 大部分数据都集中在一个区域

    \subsection{数据预处理}\label{ux6570ux636eux9884ux5904ux7406}

在这个部分，你将通过在数据上做一个合适的缩放，并检测异常点（你可以选择性移除）将数据预处理成一个更好的代表客户的形式。预处理数据是保证你在分析中能够得到显著且有意义的结果的重要环节。

    \subsubsection{练习:
特征缩放}\label{ux7ec3ux4e60-ux7279ux5f81ux7f29ux653e}

如果数据不是正态分布的，尤其是数据的平均数和中位数相差很大的时候（表示数据非常歪斜）。这时候通常用一个非线性的缩放是\href{https://github.com/czcbangkai/translations/blob/master/use_of_logarithms_in_economics/use_of_logarithms_in_economics.pdf}{很合适的}，\href{http://econbrowser.com/archives/2014/02/use-of-logarithms-in-economics}{（英文原文）}
---
尤其是对于金融数据。一种实现这个缩放的方法是使用\href{http://scipy.github.io/devdocs/generated/scipy.stats.boxcox.html}{Box-Cox
变换}，这个方法能够计算出能够最佳减小数据倾斜的指数变换方法。一个比较简单的并且在大多数情况下都适用的方法是使用自然对数。

在下面的代码单元中，你将需要实现以下功能： -
使用\texttt{np.log}函数在数据 \texttt{data}
上做一个对数缩放，然后将它的副本（不改变原始data的值）赋值给\texttt{log\_data}。
- 使用\texttt{np.log}函数在样本数据 \texttt{samples}
上做一个对数缩放，然后将它的副本赋值给\texttt{log\_samples}。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{} TODO：使用自然对数缩放数据}
         \PY{n}{log\PYZus{}data} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{data} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} TODO：使用自然对数缩放样本数据}
         \PY{n}{log\PYZus{}samples} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{samples} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} 为每一对新产生的特征制作一个散射矩阵}
         \PY{n}{pd}\PY{o}{.}\PY{n}{plotting}\PY{o}{.}\PY{n}{scatter\PYZus{}matrix}\PY{p}{(}\PY{n}{log\PYZus{}data}\PY{p}{,} \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{,} \PY{n}{diagonal} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kde}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_21_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{观察}\label{ux89c2ux5bdf}

在使用了一个自然对数的缩放之后，数据的各个特征会显得更加的正态分布。对于任意的你以前发现有相关关系的特征对，观察他们的相关关系是否还是存在的（并且尝试观察，他们的相关关系相比原来是变强了还是变弱了）。

运行下面的代码以观察样本数据在进行了自然对数转换之后如何改变了。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{c+c1}{\PYZsh{} 展示经过对数变换后的样本数据}
         \PY{n}{display}\PY{p}{(}\PY{n}{log\PYZus{}samples}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
      Fresh      Milk    Grocery    Frozen  Detergents_Paper  Delicatessen
0  8.861917  9.191259   9.166284  7.474772          8.099858      7.482682
1  9.358329  8.959312   9.404673  8.087333          8.991687      8.016318
2  8.028781  9.491073  10.048799  8.279443          9.206332      6.595781
    \end{verbatim}

    
    \subsubsection{练习:
异常值检测}\label{ux7ec3ux4e60-ux5f02ux5e38ux503cux68c0ux6d4b}

对于任何的分析，在数据预处理的过程中检测数据中的异常值都是非常重要的一步。异常值的出现会使得把这些值考虑进去后结果出现倾斜。这里有很多关于怎样定义什么是数据集中的异常值的经验法则。这里我们将使用\href{http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/}{Tukey的定义异常值的方法}：一个\emph{异常阶（outlier
step）}被定义成1.5倍的四分位距（interquartile
range，IQR）。一个数据点如果某个特征包含在该特征的IQR之外的特征，那么该数据点被认定为异常点。

在下面的代码单元中，你需要完成下面的功能： -
将指定特征的25th分位点的值分配给\texttt{Q1}。使用\texttt{np.percentile}来完成这个功能。
-
将指定特征的75th分位点的值分配给\texttt{Q3}。同样的，使用\texttt{np.percentile}来完成这个功能。
- 将指定特征的异常阶的计算结果赋值给\texttt{step}. -
选择性地通过将索引添加到\texttt{outliers}列表中，以移除异常值。

\textbf{注意：}
如果你选择移除异常值，请保证你选择的样本点不在这些移除的点当中！
一旦你完成了这些功能，数据集将存储在\texttt{good\_data}中。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} 对于每一个特征，找到值异常高或者是异常低的数据点}
         \PY{n}{outliers} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{log\PYZus{}data}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             
             \PY{c+c1}{\PYZsh{} TODO：计算给定特征的Q1（数据的25th分位点）}
              \PY{n}{Q1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{log\PYZus{}data}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} TODO：计算给定特征的Q3（数据的75th分位点）}
              \PY{n}{Q3} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{log\PYZus{}data}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{75}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} TODO：使用四分位范围计算异常阶（1.5倍的四分位距）}
              \PY{n}{step} \PY{o}{=} \PY{p}{(}\PY{n}{Q3} \PY{o}{\PYZhy{}} \PY{n}{Q1}\PY{p}{)} \PY{o}{*} \PY{l+m+mf}{1.5}
             
             \PY{c+c1}{\PYZsh{} 显示异常点}
              \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Data points considered outliers for the feature }\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{:}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{feature}\PY{p}{)}
              \PY{n}{outdatas} \PY{o}{=} \PY{n}{log\PYZus{}data}\PY{p}{[}\PY{o}{\PYZti{}}\PY{p}{(}\PY{p}{(}\PY{n}{log\PYZus{}data}\PY{p}{[}\PY{n}{feature}\PY{p}{]} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{Q1} \PY{o}{\PYZhy{}} \PY{n}{step}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{log\PYZus{}data}\PY{p}{[}\PY{n}{feature}\PY{p}{]} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{Q3} \PY{o}{+} \PY{n}{step}\PY{p}{)}\PY{p}{)}\PY{p}{]}
              \PY{n}{outliers}\PY{o}{.}\PY{n}{extend}\PY{p}{(}\PY{n}{outdatas}\PY{o}{.}\PY{n}{index}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}
              \PY{n}{display}\PY{p}{(}\PY{n}{log\PYZus{}data}\PY{p}{[}\PY{o}{\PYZti{}}\PY{p}{(}\PY{p}{(}\PY{n}{log\PYZus{}data}\PY{p}{[}\PY{n}{feature}\PY{p}{]} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{Q1} \PY{o}{\PYZhy{}} \PY{n}{step}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{log\PYZus{}data}\PY{p}{[}\PY{n}{feature}\PY{p}{]} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{Q3} \PY{o}{+} \PY{n}{step}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
             
         \PY{c+c1}{\PYZsh{} 可选：选择你希望移除的数据点的索引}
         \PY{c+c1}{\PYZsh{} outliers  = []}
         \PY{n}{outliers} \PY{o}{=} \PY{p}{[}\PY{n}{val} \PY{k}{for} \PY{n}{val} \PY{o+ow}{in} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{set}\PY{p}{(}\PY{n}{outliers}\PY{p}{)}\PY{p}{)} \PY{k}{if} \PY{n}{outliers}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{n}{val}\PY{p}{)}\PY{o}{\PYZgt{}}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{]}
         \PY{k}{print} \PY{n}{outliers}
         \PY{c+c1}{\PYZsh{} 如果选择了的话，移除异常点}
         \PY{n}{good\PYZus{}data} \PY{o}{=} \PY{n}{log\PYZus{}data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{log\PYZus{}data}\PY{o}{.}\PY{n}{index}\PY{p}{[}\PY{n}{outliers}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{n}{drop} \PY{o}{=} \PY{n+nb+bp}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Data points considered outliers for the feature 'Fresh':

    \end{Verbatim}

    
    \begin{verbatim}
        Fresh       Milk    Grocery    Frozen  Detergents_Paper  Delicatessen
65   4.454347   9.950371  10.732672  3.610918         10.095429      7.261225
66   2.302585   7.336286   8.911665  5.170484          8.151622      3.332205
81   5.393628   9.163354   9.575261  5.648974          8.964312      5.056246
95   1.386294   7.979681   8.740817  6.089045          5.411646      6.565265
96   3.178054   7.869784   9.001962  4.983607          8.262301      5.384495
128  4.948760   9.087947   8.249052  4.962845          6.968850      1.386294
171  5.303305  10.160569   9.894295  6.480045          9.079548      8.740497
193  5.198497   8.156510   9.918031  6.866933          8.633909      6.502790
218  2.944439   8.923325   9.629445  7.159292          8.475954      8.759826
304  5.087596   8.917445  10.117550  6.426488          9.374498      7.787797
305  5.497168   9.468079   9.088512  6.684612          8.271293      5.356586
338  1.386294   5.811141   8.856803  9.655154          2.772589      6.311735
353  4.770685   8.742734   9.961945  5.433722          9.069122      7.013915
355  5.252273   6.590301   7.607381  5.505332          5.220356      4.852030
357  3.637586   7.151485  10.011130  4.927254          8.817001      4.709530
412  4.584967   8.190354   9.425532  4.595120          7.996654      4.143135
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
Data points considered outliers for the feature 'Milk':

    \end{Verbatim}

    
    \begin{verbatim}
         Fresh       Milk    Grocery    Frozen  Detergents_Paper  Delicatessen
86   10.040027  11.205027  10.377078  6.895683          9.907031      6.806829
98    6.222576   4.727388   6.658011  6.797940          4.043051      4.890349
154   6.434547   4.025352   4.927254  4.330733          2.079442      2.197225
356  10.029547   4.905275   5.389072  8.057694          2.302585      6.308098
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
Data points considered outliers for the feature 'Grocery':

    \end{Verbatim}

    
    \begin{verbatim}
        Fresh      Milk   Grocery    Frozen  Detergents_Paper  Delicatessen
75   9.923241  7.037028  1.386294  8.391176          1.386294      6.883463
154  6.434547  4.025352  4.927254  4.330733          2.079442      2.197225
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
Data points considered outliers for the feature 'Frozen':

    \end{Verbatim}

    
    \begin{verbatim}
         Fresh      Milk    Grocery     Frozen  Detergents_Paper  Delicatessen
38    8.432071  9.663325   9.723763   3.526361          8.847504      6.073045
57    8.597482  9.203718   9.257987   3.663562          8.932345      7.156956
65    4.454347  9.950371  10.732672   3.610918         10.095429      7.261225
145  10.000614  9.034200  10.457171   3.761200          9.440817      8.396381
175   7.759614  8.967759   9.382191   3.970292          8.342125      7.437206
264   6.979145  9.177817   9.645105   4.127134          8.696343      7.143618
325  10.395681  9.728241   9.519808  11.016496          7.149132      8.632306
420   8.402231  8.569216   9.490091   3.258097          8.827468      7.239933
429   9.060447  7.467942   8.183397   3.871201          4.442651      7.824846
439   7.933080  7.437795   7.828436   4.189655          6.169611      3.970292
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
Data points considered outliers for the feature 'Detergents\_Paper':

    \end{Verbatim}

    
    \begin{verbatim}
        Fresh      Milk   Grocery    Frozen  Detergents_Paper  Delicatessen
75   9.923241  7.037028  1.386294  8.391176          1.386294      6.883463
161  9.428270  6.293419  5.648974  6.996681          1.386294      7.711549
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
Data points considered outliers for the feature 'Delicatessen':

    \end{Verbatim}

    
    \begin{verbatim}
         Fresh       Milk    Grocery     Frozen  Detergents_Paper  \
66    2.302585   7.336286   8.911665   5.170484          8.151622   
109   7.249215   9.724959  10.274603   6.513230          6.729824   
128   4.948760   9.087947   8.249052   4.962845          6.968850   
137   8.035279   8.997271   9.021961   6.495266          6.582025   
142  10.519673   8.875287   9.018453   8.005033          3.044522   
154   6.434547   4.025352   4.927254   4.330733          2.079442   
183  10.514557  10.690831   9.912001  10.506026          5.480639   
184   5.793014   6.823286   8.457655   4.317488          5.814131   
187   7.799343   8.987572   9.192176   8.743532          8.149024   
203   6.369901   6.530878   7.703910   6.152733          6.861711   
233   6.872128   8.514189   8.106816   6.843750          6.016157   
285  10.602989   6.463029   8.188967   6.949856          6.079933   
289  10.663990   5.659482   6.156979   7.236339          3.496508   
343   7.432484   8.848653  10.177970   7.284135          9.646658   

     Delicatessen  
66       3.332205  
109      1.386294  
128      1.386294  
137      3.610918  
142      1.386294  
154      2.197225  
183     10.777789  
184      2.484907  
187      1.386294  
203      2.944439  
233      2.079442  
285      2.944439  
289      3.135494  
343      3.637586  
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
[128L, 154L, 65L, 66L, 75L]

    \end{Verbatim}

    \subsubsection{问题 4}\label{ux95eeux9898-4}

\emph{请列出所有在多于一个特征下被看作是异常的数据点。这些点应该被从数据集中移除吗？为什么？把你认为需要移除的数据点全部加入到到\texttt{outliers}变量中。}

    \textbf{回答:} - 128L, 154L, 65L, 66L,
75L这几个数据多于一个特征下被看作了异常点，应该被移除 -
因为异常点的存在会导致训练的误差增大，所以需要去除

    \subsection{特征转换}\label{ux7279ux5f81ux8f6cux6362}

在这个部分中你将使用主成分分析（PCA）来分析批发商客户数据的内在结构。由于使用PCA在一个数据集上会计算出最大化方差的维度，我们将找出哪一个特征组合能够最好的描绘客户。

    \subsubsection{练习:
主成分分析（PCA）}\label{ux7ec3ux4e60-ux4e3bux6210ux5206ux5206ux6790pca}

既然数据被缩放到一个更加正态分布的范围中并且我们也移除了需要移除的异常点，我们现在就能够在\texttt{good\_data}上使用PCA算法以发现数据的哪一个维度能够最大化特征的方差。除了找到这些维度，PCA也将报告每一个维度的\emph{解释方差比（explained
variance
ratio）}-\/-这个数据有多少方差能够用这个单独的维度来解释。注意PCA的一个组成部分（维度）能够被看做这个空间中的一个新的``特征''，但是它是原来数据中的特征构成的。

在下面的代码单元中，你将要实现下面的功能： -
导入\texttt{sklearn.decomposition.PCA}并且将\texttt{good\_data}用PCA并且使用6个维度进行拟合后的结果保存到\texttt{pca}中。
-
使用\texttt{pca.transform}将\texttt{log\_samples}进行转换，并将结果存储到\texttt{pca\_samples}中。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{} TODO：通过在good\PYZus{}data上使用PCA，将其转换成和当前特征数一样多的维度}
         \PY{k+kn}{from} \PY{n+nn}{sklearn.decomposition} \PY{k+kn}{import} \PY{n}{PCA}
         \PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
         \PY{n}{pca}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{good\PYZus{}data}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} TODO：使用上面的PCA拟合将变换施加在log\PYZus{}samples上}
         \PY{n}{pca\PYZus{}samples} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{log\PYZus{}samples}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} 生成PCA的结果图}
         \PY{n}{pca\PYZus{}results} \PY{o}{=} \PY{n}{vs}\PY{o}{.}\PY{n}{pca\PYZus{}results}\PY{p}{(}\PY{n}{good\PYZus{}data}\PY{p}{,} \PY{n}{pca}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_30_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{问题 5}\label{ux95eeux9898-5}

\emph{数据的第一个和第二个主成分} \textbf{总共}
\emph{表示了多少的方差？}
前四个主成分呢？使用上面提供的可视化图像，讨论从用户花费的角度来看前四个\textbf{主要成分}的消费行为最能代表哪种类型的客户并给出你做出判断的理由。

\textbf{提示：}
某一特定维度上的正向增长对应\textbf{正权}特征的\textbf{增长}和\textbf{负权}特征的\textbf{减少}。增长和减少的速率和每个特征的权重相关。\href{https://onlinecourses.science.psu.edu/stat505/node/54}{参考资料(英文)}。

    \textbf{回答:} 总共表示了0.7085的方差 - 前四个表示了0.9311的方差

\begin{itemize}
\tightlist
\item
  表示零售商和咖啡店，从第一个维度来看，可以分成两部分，第一部分Milk、Grocery和DP他们是负权特征，彼此之间是相关性最强，第二个维度上，Fresh、Frozen和Delicatessen彼此之间相关性最强，第三个维度体现了Frozen和Delicatessen的相关性，第四个维度体现了Fresh和Delicatessen的相关性，由此可见6个特征被主要分成了2组，其中Milk、Grocery、DP代表的是咖啡店，Fresh、Frozen、Delicatessen代表的是零售商
\end{itemize}

    \subsubsection{观察}\label{ux89c2ux5bdf}

运行下面的代码，查看经过对数转换的样本数据在进行一个6个维度的主成分分析（PCA）之后会如何改变。观察样本数据的前四个维度的数值。考虑这和你初始对样本点的解释是否一致。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} 展示经过PCA转换的sample log\PYZhy{}data}
         \PY{n}{display}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{pca\PYZus{}samples}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{n}{pca\PYZus{}results}\PY{o}{.}\PY{n}{index}\PY{o}{.}\PY{n}{values}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
   Dimension 1  Dimension 2  Dimension 3  Dimension 4  Dimension 5  \
0      -1.7837      -0.8112       0.2298      -0.0154       0.1148   
1      -2.3494      -1.7385       0.2247       0.2711      -0.6018   
2      -2.9861      -0.3758       0.3013       1.5540       0.1934   

   Dimension 6  
0      -0.2111  
1      -0.0192  
2       0.1232  
    \end{verbatim}

    
    \subsubsection{练习：降维}\label{ux7ec3ux4e60ux964dux7ef4}

当使用主成分分析的时候，一个主要的目的是减少数据的维度，这实际上降低了问题的复杂度。当然降维也是需要一定代价的：更少的维度能够表示的数据中的总方差更少。因为这个，\emph{累计解释方差比（cumulative
explained variance
ratio）}对于我们确定这个问题需要多少维度非常重要。另外，如果大部分的方差都能够通过两个或者是三个维度进行表示的话，降维之后的数据能够被可视化。

在下面的代码单元中，你将实现下面的功能： -
将\texttt{good\_data}用两个维度的PCA进行拟合，并将结果存储到\texttt{pca}中去。
-
使用\texttt{pca.transform}将\texttt{good\_data}进行转换，并将结果存储在\texttt{reduced\_data}中。
-
使用\texttt{pca.transform}将\texttt{log\_samples}进行转换，并将结果存储在\texttt{pca\_samples}中。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{} TODO：通过在good data上进行PCA，将其转换成两个维度}
         \PY{k+kn}{from} \PY{n+nn}{sklearn.decomposition} \PY{k+kn}{import}  \PY{n}{PCA}
         \PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{pca}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{good\PYZus{}data}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} TODO：使用上面训练的PCA将good data进行转换}
         \PY{n}{reduced\PYZus{}data} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{good\PYZus{}data}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} TODO：使用上面训练的PCA将log\PYZus{}samples进行转换}
         \PY{n}{pca\PYZus{}samples} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{log\PYZus{}samples}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} 为降维后的数据创建一个DataFrame}
         \PY{n}{reduced\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{reduced\PYZus{}data}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dimension 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dimension 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \subsubsection{观察}\label{ux89c2ux5bdf}

运行以下代码观察当仅仅使用两个维度进行PCA转换后，这个对数样本数据将怎样变化。观察这里的结果与一个使用六个维度的PCA转换相比较时，前两维的数值是保持不变的。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{} 展示经过两个维度的PCA转换之后的样本log\PYZhy{}data}
         \PY{n}{display}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{pca\PYZus{}samples}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dimension 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dimension 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
   Dimension 1  Dimension 2
0      -1.7837      -0.8112
1      -2.3494      -1.7385
2      -2.9861      -0.3758
    \end{verbatim}

    
    \subsection{可视化一个双标图（Biplot）}\label{ux53efux89c6ux5316ux4e00ux4e2aux53ccux6807ux56febiplot}

双标图是一个散点图，每个数据点的位置由它所在主成分的分数确定。坐标系是主成分（这里是\texttt{Dimension\ 1}
和
\texttt{Dimension\ 2}）。此外，双标图还展示出初始特征在主成分上的投影。一个双标图可以帮助我们理解降维后的数据，发现主成分和初始特征之间的关系。

运行下面的代码来创建一个降维后数据的双标图。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{c+c1}{\PYZsh{} Create a biplot}
         \PY{n}{vs}\PY{o}{.}\PY{n}{biplot}\PY{p}{(}\PY{n}{good\PYZus{}data}\PY{p}{,} \PY{n}{reduced\PYZus{}data}\PY{p}{,} \PY{n}{pca}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}17}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0xbc11588>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_40_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{观察}\label{ux89c2ux5bdf}

一旦我们有了原始特征的投影（红色箭头），就能更加容易的理解散点图每个数据点的相对位置。

在这个双标图中，哪些初始特征与第一个主成分有强关联？哪些初始特征与第二个主成分相关联？你观察到的是否与之前得到的
pca\_results 图相符？

    \subsection{聚类}\label{ux805aux7c7b}

在这个部分，你讲选择使用K-Means聚类算法或者是高斯混合模型聚类算法以发现数据中隐藏的客户分类。然后，你将从簇中恢复一些特定的关键数据点，通过将它们转换回原始的维度和规模，从而理解他们的含义。

    \subsubsection{问题 6}\label{ux95eeux9898-6}

\emph{使用K-Means聚类算法的优点是什么？使用高斯混合模型聚类算法的优点是什么？基于你现在对客户数据的观察结果，你选用了这两个算法中的哪一个，为什么？}

    \textbf{回答:} -
使用K-Means，算法简单，容易理解。计算量不大，收敛快。可以很方便的进行分布式计算。
-
使用高斯混合模型聚类，优点是投影后样本点不是得到一个确定的分类标记，而是得到每个类的概率。针对服从正态分布、偏态分布的数据有比较好的效果。
-
选择了高斯混合模型聚类，因为处理过的数据基本服从正态分布，所以可能会有较好的效果

    \subsubsection{练习:
创建聚类}\label{ux7ec3ux4e60-ux521bux5efaux805aux7c7b}

针对不同情况，有些问题你需要的聚类数目可能是已知的。但是在聚类数目不作为一个\textbf{先验}知道的情况下，我们并不能够保证某个聚类的数目对这个数据是最优的，因为我们对于数据的结构（如果存在的话）是不清楚的。但是，我们可以通过计算每一个簇中点的\textbf{轮廓系数}来衡量聚类的质量。数据点的\href{http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html}{轮廓系数}衡量了它与分配给他的簇的相似度，这个值范围在-1（不相似）到1（相似）。\textbf{平均}轮廓系数为我们提供了一种简单地度量聚类质量的方法。

在接下来的代码单元中，你将实现下列功能： -
在\texttt{reduced\_data}上使用一个聚类算法，并将结果赋值到\texttt{clusterer}，需要设置
\texttt{random\_state} 使得结果可以复现。 -
使用\texttt{clusterer.predict}预测\texttt{reduced\_data}中的每一个点的簇，并将结果赋值到\texttt{preds}。
- 使用算法的某个属性值找到聚类中心，并将它们赋值到\texttt{centers}。 -
预测\texttt{pca\_samples}中的每一个样本点的类别并将结果赋值到\texttt{sample\_preds}。
-
导入sklearn.metrics.silhouette\_score包并计算\texttt{reduced\_data}相对于\texttt{preds}的轮廓系数。
- 将轮廓系数赋值给\texttt{score}并输出结果。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{c+c1}{\PYZsh{} TODO：在降维后的数据上使用你选择的聚类算法}
         \PY{k+kn}{from} \PY{n+nn}{sklearn.mixture} \PY{k+kn}{import} \PY{n}{GaussianMixture}
         \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{silhouette\PYZus{}score}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{xrange}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{:}
             \PY{n}{clusterer} \PY{o}{=} \PY{n}{GaussianMixture}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{i}\PY{p}{,}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
             \PY{n}{clusterer}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{reduced\PYZus{}data}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} TODO：预测每一个点的簇}
             \PY{n}{preds} \PY{o}{=} \PY{n}{clusterer}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{reduced\PYZus{}data}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} TODO：找到聚类中心}
             \PY{n}{centers} \PY{o}{=} \PY{n}{clusterer}\PY{o}{.}\PY{n}{means\PYZus{}}
         
             \PY{c+c1}{\PYZsh{} TODO：预测在每一个转换后的样本点的类}
             \PY{n}{sample\PYZus{}preds} \PY{o}{=} \PY{n}{clusterer}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{pca\PYZus{}samples}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} TODO：计算选择的类别的平均轮廓系数（mean silhouette coefficient）}
             \PY{n}{score} \PY{o}{=} \PY{n}{silhouette\PYZus{}score}\PY{p}{(}\PY{n}{reduced\PYZus{}data}\PY{p}{,}\PY{n}{preds}\PY{p}{)}
             \PY{k}{print} \PY{n}{score}
         
         
         \PY{n}{clusterer} \PY{o}{=} \PY{n}{GaussianMixture}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{clusterer}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{reduced\PYZus{}data}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} TODO：预测每一个点的簇}
         \PY{n}{preds} \PY{o}{=} \PY{n}{clusterer}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{reduced\PYZus{}data}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} TODO：找到聚类中心}
         \PY{n}{centers} \PY{o}{=} \PY{n}{clusterer}\PY{o}{.}\PY{n}{means\PYZus{}}
         
         \PY{c+c1}{\PYZsh{} TODO：预测在每一个转换后的样本点的类}
         \PY{n}{sample\PYZus{}preds} \PY{o}{=} \PY{n}{clusterer}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{pca\PYZus{}samples}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} TODO：计算选择的类别的平均轮廓系数（mean silhouette coefficient）}
         \PY{n}{score} \PY{o}{=} \PY{n}{silhouette\PYZus{}score}\PY{p}{(}\PY{n}{reduced\PYZus{}data}\PY{p}{,}\PY{n}{preds}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
0.422922850609
0.370495744444
0.330340504197
0.309626504416
0.330147227158
0.283632396906

    \end{Verbatim}

    \subsubsection{问题 7}\label{ux95eeux9898-7}

\emph{汇报你尝试的不同的聚类数对应的轮廓系数。在这些当中哪一个聚类的数目能够得到最佳的轮廓系数？}

    \textbf{回答:}

0.422922850609

0.370495744444

0.330340504197

0.309626504416

0.330147227158

0.283632396906

2聚类得到了最佳的轮廓系数

    \subsubsection{聚类可视化}\label{ux805aux7c7bux53efux89c6ux5316}

一旦你选好了通过上面的评价函数得到的算法的最佳聚类数目，你就能够通过使用下面的代码块可视化来得到的结果。作为实验，你可以试着调整你的聚类算法的聚类的数量来看一下不同的可视化结果。但是你提供的最终的可视化图像必须和你选择的最优聚类数目一致。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} 从已有的实现中展示聚类的结果}
         \PY{n}{vs}\PY{o}{.}\PY{n}{cluster\PYZus{}results}\PY{p}{(}\PY{n}{reduced\PYZus{}data}\PY{p}{,} \PY{n}{preds}\PY{p}{,} \PY{n}{centers}\PY{p}{,} \PY{n}{pca\PYZus{}samples}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_50_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{练习:
数据恢复}\label{ux7ec3ux4e60-ux6570ux636eux6062ux590d}

上面的可视化图像中提供的每一个聚类都有一个中心点。这些中心（或者叫平均点）并不是数据中真实存在的点，但是是所有预测在这个簇中的数据点的\emph{平均}。对于创建客户分类的问题，一个簇的中心对应于\emph{那个分类的平均用户}。因为这个数据现在进行了降维并缩放到一定的范围，我们可以通过施加一个反向的转换恢复这个点所代表的用户的花费。

在下面的代码单元中，你将实现下列的功能： -
使用\texttt{pca.inverse\_transform}将\texttt{centers}
反向转换，并将结果存储在\texttt{log\_centers}中。 -
使用\texttt{np.log}的反函数\texttt{np.exp}反向转换\texttt{log\_centers}并将结果存储到\texttt{true\_centers}中。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{} TODO：反向转换中心点}
         \PY{n}{log\PYZus{}centers} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}\PY{n}{centers}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} TODO：对中心点做指数转换}
         \PY{n}{true\PYZus{}centers} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{log\PYZus{}centers}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} 显示真实的中心点}
         \PY{n}{segments} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Segment \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{centers}\PY{p}{)}\PY{p}{)}\PY{p}{]}
         \PY{n}{true\PYZus{}centers} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{true\PYZus{}centers}\PY{p}{)}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{true\PYZus{}centers}\PY{o}{.}\PY{n}{index} \PY{o}{=} \PY{n}{segments}
         \PY{n}{display}\PY{p}{(}\PY{n}{true\PYZus{}centers}\PY{p}{)}
         \PY{n}{display}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
            Fresh    Milk  Grocery  Frozen  Detergents_Paper  Delicatessen
Segment 0  8937.0  2100.0   2747.0  2076.0             353.0         731.0
Segment 1  3604.0  7878.0  12261.0   878.0            4695.0         976.0
    \end{verbatim}

    
    
    \begin{verbatim}
               Fresh          Milk       Grocery        Frozen  \
count     440.000000    440.000000    440.000000    440.000000   
mean    12000.297727   5796.265909   7951.277273   3071.931818   
std     12647.328865   7380.377175   9503.162829   4854.673333   
min         3.000000     55.000000      3.000000     25.000000   
25%      3127.750000   1533.000000   2153.000000    742.250000   
50%      8504.000000   3627.000000   4755.500000   1526.000000   
75%     16933.750000   7190.250000  10655.750000   3554.250000   
max    112151.000000  73498.000000  92780.000000  60869.000000   

       Detergents_Paper  Delicatessen  
count        440.000000    440.000000  
mean        2881.493182   1524.870455  
std         4767.854448   2820.105937  
min            3.000000      3.000000  
25%          256.750000    408.250000  
50%          816.500000    965.500000  
75%         3922.000000   1820.250000  
max        40827.000000  47943.000000  
    \end{verbatim}

    
    \subsubsection{问题 8}\label{ux95eeux9898-8}

考虑上面的代表性数据点在每一个产品类型的花费总数，\emph{你认为这些客户分类代表了哪类客户？为什么？}需要参考在项目最开始得到的统计值来给出理由。

\textbf{提示：}
一个被分到\texttt{\textquotesingle{}Cluster\ X\textquotesingle{}}的客户最好被用
\texttt{\textquotesingle{}Segment\ X\textquotesingle{}}中的特征集来标识的企业类型表示。

    \textbf{回答:} - Cluster 0更像是零售商，因为他们的中心点Segment
0的各项特征值都是50\%左右，低于平均值，不存在某个特征特别高，而量又没有特别大，符合零售商什么都卖，但是入货不算多的特点
- Cluster 1更像是咖啡店，并且他们的中心点Segment
1的值牛奶和、杂物和清洁纸的量超过了75\%而且超过了平均数，而其他的数量较少，咖啡店牛奶和纸巾的消耗必然很多，所以更有可能是咖啡店

    \subsubsection{问题 9}\label{ux95eeux9898-9}

\emph{对于每一个样本点 } \textbf{问题 8}
\emph{中的哪一个分类能够最好的表示它？你之前对样本的预测和现在的结果相符吗？}

运行下面的代码单元以找到每一个样本点被预测到哪一个簇中去。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{c+c1}{\PYZsh{} 显示预测结果}
         \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{pred} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{sample\PYZus{}preds}\PY{p}{)}\PY{p}{:}
             \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sample point}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{i}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{predicted to be in Cluster}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{pred}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Sample point 0 predicted to be in Cluster 1
Sample point 1 predicted to be in Cluster 1
Sample point 2 predicted to be in Cluster 1

    \end{Verbatim}

    \textbf{回答:} 三个样本都被分到了聚类1

和预测不符合

    \subsection{结论}\label{ux7ed3ux8bba}

在最后一部分中，你要学习如何使用已经被分类的数据。首先，你要考虑不同组的客户\textbf{客户分类}，针对不同的派送策略受到的影响会有什么不同。其次，你要考虑到，每一个客户都被打上了标签（客户属于哪一个分类）可以给客户数据提供一个多一个特征。最后，你会把客户分类与一个数据中的隐藏变量做比较，看一下这个分类是否辨识了特定的关系。

    \subsubsection{问题 10}\label{ux95eeux9898-10}

在对他们的服务或者是产品做细微的改变的时候，公司经常会使用\href{https://en.wikipedia.org/wiki/A/B_testing}{A/B
tests}以确定这些改变会对客户产生积极作用还是消极作用。这个批发商希望考虑将他的派送服务从每周5天变为每周3天，但是他只会对他客户当中对此有积极反馈的客户采用。\emph{这个批发商应该如何利用客户分类来知道哪些客户对它的这个派送策略的改变有积极的反馈，如果有的话？你需要给出在这个情形下A/B
测试具体的实现方法，以及最终得出结论的依据是什么？}\\
\textbf{提示：}
我们能假设这个改变对所有的客户影响都一致吗？我们怎样才能够确定它对于哪个类型的客户影响最大？

    \textbf{回答：} 1.先从目前分类中
各筛选一部分用户修改为每周3天的派送策略,
2.获取用户反馈，之后进行反馈分析评估

    \subsubsection{问题 11}\label{ux95eeux9898-11}

通过聚类技术，我们能够将原有的没有标记的数据集中的附加结构分析出来。因为每一个客户都有一个最佳的划分（取决于你选择使用的聚类算法），我们可以把\emph{用户分类}作为数据的一个\href{https://en.wikipedia.org/wiki/Feature_learning\#Unsupervised_feature_learning}{\textbf{工程特征}}。假设批发商最近迎来十位新顾客，并且他已经为每位顾客每个产品类别年度采购额进行了预估。进行了这些估算之后，批发商该如何运用它的预估和\textbf{非监督学习的结果}来对这十个新的客户进行更好的预测？

\textbf{提示：}在下面的代码单元中，我们提供了一个已经做好聚类的数据（聚类结果为数据中的cluster属性），我们将在这个数据集上做一个小实验。尝试运行下面的代码看看我们尝试预测`Region'的时候，如果存在聚类特征'cluster'与不存在相比对最终的得分会有什么影响？这对你有什么启发？

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.ensemble} \PY{k+kn}{import} \PY{n}{RandomForestClassifier}
         \PY{k+kn}{from} \PY{n+nn}{sklearn.model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
         
         \PY{c+c1}{\PYZsh{} 读取包含聚类结果的数据}
         \PY{n}{cluster\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cluster.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{y} \PY{o}{=} \PY{n}{cluster\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Region}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{X} \PY{o}{=} \PY{n}{cluster\PYZus{}data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Region}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} 划分训练集测试集}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{24}\PY{p}{)}
         
         \PY{n}{clf} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{24}\PY{p}{)}
         \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{使用cluster特征的得分}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} 移除cluster特征}
         \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
         \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cluster}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
         \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
         \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cluster}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
         \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{k}{print} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{不使用cluster特征的得分}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
使用cluster特征的得分 0.666666666667
不使用cluster特征的得分 0.64367816092

    \end{Verbatim}

    \textbf{回答：} cluster的存在会使得分变高，有可能预测会变得更准确

    \subsubsection{可视化内在的分布}\label{ux53efux89c6ux5316ux5185ux5728ux7684ux5206ux5e03}

在这个项目的开始，我们讨论了从数据集中移除\texttt{\textquotesingle{}Channel\textquotesingle{}}和\texttt{\textquotesingle{}Region\textquotesingle{}}特征，这样在分析过程中我们就会着重分析用户产品类别。通过重新引入\texttt{Channel}这个特征到数据集中，并施加和原来数据集同样的PCA变换的时候我们将能够发现数据集产生一个有趣的结构。

运行下面的代码单元以查看哪一个数据点在降维的空间中被标记为\texttt{\textquotesingle{}HoReCa\textquotesingle{}}
(旅馆/餐馆/咖啡厅)或者\texttt{\textquotesingle{}Retail\textquotesingle{}}。另外，你将发现样本点在图中被圈了出来，用以显示他们的标签。

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{c+c1}{\PYZsh{} 根据‘Channel‘数据显示聚类的结果}
         \PY{n}{vs}\PY{o}{.}\PY{n}{channel\PYZus{}results}\PY{p}{(}\PY{n}{reduced\PYZus{}data}\PY{p}{,} \PY{n}{outliers}\PY{p}{,} \PY{n}{pca\PYZus{}samples}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{问题 12}\label{ux95eeux9898-12}

\emph{你选择的聚类算法和聚类点的数目，与内在的旅馆/餐馆/咖啡店和零售商的分布相比，有足够好吗？根据这个分布有没有哪个簇能够刚好划分成'零售商'或者是'旅馆/饭店/咖啡馆'？你觉得这个分类和前面你对于用户分类的定义是一致的吗？}

    \textbf{回答：} - 不够好 - Dimension1 为 -0.3 的时候刚好分开 - 不一致

    \begin{quote}
\textbf{注意}:
当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython
Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出\textbf{File
-\textgreater{} Download as -\textgreater{} HTML (.html)}把这个 HTML
和这个 iPython notebook 一起做为你的作业提交。
\end{quote}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
